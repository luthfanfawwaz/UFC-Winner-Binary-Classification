{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.util as utils\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import hashlib\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Model-Ready datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(config: dict) -> pd.DataFrame:\n",
    "    # Load train set\n",
    "    x_train = utils.pickle_load(config[\"train_set_modelready_path\"][0])\n",
    "    y_train = utils.pickle_load(config[\"train_set_modelready_path\"][1])\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "def load_valid(config: dict) -> pd.DataFrame:\n",
    "    # Load valid set\n",
    "    x_valid = utils.pickle_load(config[\"valid_set_modelready_path\"][0])\n",
    "    y_valid = utils.pickle_load(config[\"valid_set_modelready_path\"][1])\n",
    "\n",
    "    return x_valid, y_valid\n",
    "\n",
    "def load_test(config: dict) -> pd.DataFrame:\n",
    "    # Load tets set\n",
    "    x_test = utils.pickle_load(config[\"test_set_modelready_path\"][0])\n",
    "    y_test = utils.pickle_load(config[\"test_set_modelready_path\"][1])\n",
    "\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_train(config)\n",
    "x_valid, y_valid = load_valid(config)\n",
    "x_test, y_test = load_test(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our EDA we know that our target is leaning toward red fighter at 58.365%. Which means if we take all of our prediction to be red, it's likely to have 58% accuracy.\n",
    "\n",
    "We will set this to be our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we want to maintain our interpretability, let's choose Decision Tree as our base model \n",
    "dtc = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, min_samples_split=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, min_samples_split=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', min_samples_split=10)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.51      0.47       305\n",
      "           1       0.60      0.52      0.56       429\n",
      "\n",
      "    accuracy                           0.52       734\n",
      "   macro avg       0.52      0.52      0.51       734\n",
      "weighted avg       0.53      0.52      0.52       734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, our first model didn't perform so well. It even performed worse that our baseline. It was already suggested by our EDA.\n",
    "\n",
    "Let's try some more model, now in the form of function so we can access it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object() -> list:\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Creating model objects.\")\n",
    "\n",
    "    # Create model objects\n",
    "    lgr = LogisticRegression()\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    rfc = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "    xgb = XGBClassifier()\n",
    "\n",
    "    # Create list of model\n",
    "    list_of_model = [\n",
    "        { \"model_name\": lgr.__class__.__name__, \"model_object\": lgr, \"model_uid\": \"\", \"performance\": \"\", \"f1_score_avg\": \"\"},\n",
    "        { \"model_name\": dtc.__class__.__name__, \"model_object\": dtc, \"model_uid\": \"\", \"performance\": \"\", \"f1_score_avg\": \"\"},\n",
    "        { \"model_name\": rfc.__class__.__name__, \"model_object\": rfc, \"model_uid\": \"\", \"performance\": \"\", \"f1_score_avg\": \"\"},\n",
    "        { \"model_name\": knn.__class__.__name__, \"model_object\": knn, \"model_uid\": \"\", \"performance\": \"\", \"f1_score_avg\": \"\"},\n",
    "        { \"model_name\": xgb.__class__.__name__, \"model_object\": xgb, \"model_uid\": \"\", \"performance\": \"\", \"f1_score_avg\": \"\"}\n",
    "    ]\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Model objects created.\")\n",
    "\n",
    "    # Return the list of model\n",
    "    return list_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_params(model_name: str) -> dict:\n",
    "    # Define models parameters\n",
    "    dist_params_xgb = {\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500]\n",
    "    }\n",
    "    dist_params_dtc = {\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"min_samples_split\" : [2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_samples_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "    dist_params_knn = {\n",
    "        \"algorithm\" : [\"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "        \"n_neighbors\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "        \"leaf_size\" : [2, 3, 4, 5, 6, 10, 15, 20, 25],\n",
    "    }\n",
    "    dist_params_lgr = {\n",
    "        \"penalty\" : [\"l1\", \"l2\", \"none\"],\n",
    "        \"C\" : [0.01, 0.05, 0.10, 0.15, 0.20, 0.30, 0.60, 0.90, 0.99],\n",
    "        \"solver\" : [\"saga\"],\n",
    "        \"max_iter\" : [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "    }\n",
    "    dist_params_rfc = {\n",
    "        \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"n_estimators\" : [50, 100, 200, 300, 400, 500],\n",
    "        \"min_samples_split\" : [2, 4, 6, 10, 15, 20, 25],\n",
    "        \"min_samples_leaf\" : [1, 2, 4, 6, 10, 15, 20, 25]\n",
    "    }\n",
    "\n",
    "    # Make all models parameters into one\n",
    "    dist_params = {\n",
    "        \"XGBClassifier\": dist_params_xgb,\n",
    "        \"DecisionTreeClassifier\": dist_params_dtc,\n",
    "        \"KNeighborsClassifier\": dist_params_knn,\n",
    "        \"LogisticRegression\": dist_params_lgr,\n",
    "        \"RandomForestClassifier\": dist_params_rfc\n",
    "    }\n",
    "\n",
    "    # Return distribution of model parameters\n",
    "    return dist_params[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams_tuning(list_of_model: dict) -> list:\n",
    "    # Create copy of current best baseline model\n",
    "    list_of_model = copy.deepcopy(list_of_model)\n",
    "    list_of_rsc = list()\n",
    "\n",
    "    for model in list_of_model:\n",
    "        # Create model's parameter distribution\n",
    "        dist_params = create_dist_params(model[\"model_name\"])\n",
    "\n",
    "        # Create model object\n",
    "        model_rsc = RandomizedSearchCV(model[\"model_object\"], dist_params, n_jobs = -1)\n",
    "        model = {\n",
    "            \"model_name\": model[\"model_name\"],\n",
    "            \"model_object\": model_rsc,\n",
    "            \"model_uid\": \"\", \n",
    "            \"performance\": \"\", \n",
    "            \"f1_score_avg\": \"\"\n",
    "        }\n",
    "        list_of_rsc.append(model)\n",
    "    \n",
    "    # Return model object\n",
    "    return list_of_rsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(\n",
    "        x_train: pd.DataFrame, \n",
    "        y_train: pd.DataFrame, \n",
    "        x_valid: pd.DataFrame, \n",
    "        y_valid: pd.DataFrame,\n",
    "        data_configuration:str,\n",
    "        hyperparams_tuning: bool = False\n",
    "    ):\n",
    "    \n",
    "    x_train = {\n",
    "        data_configuration : x_train\n",
    "    }\n",
    "\n",
    "    y_train = {\n",
    "        data_configuration : y_train\n",
    "    }\n",
    "\n",
    "    # Variabel to store trained models\n",
    "    list_of_trained_model = list()\n",
    "\n",
    "    # Training for every data configuration\n",
    "    for config_data in x_train:\n",
    "        # Debug message\n",
    "        utils.print_debug(\"Training model based on configuration data: {}\".format(config_data))\n",
    "\n",
    "        # Create model objects\n",
    "        if hyperparams_tuning:\n",
    "            list_of_model = get_hyperparams_tuning(create_model_object())\n",
    "        else:\n",
    "            list_of_model = create_model_object()\n",
    "\n",
    "        # Variabel to store tained model\n",
    "        trained_model = list()\n",
    "\n",
    "        # Load train data based on its configuration\n",
    "        x_train_data = x_train[config_data]\n",
    "        y_train_data = y_train[config_data]\n",
    "\n",
    "        # Train each model by current dataset configuration\n",
    "        for model in list_of_model:\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Training model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Training\n",
    "            training_time = utils.time_stamp()\n",
    "            model[\"model_object\"].fit(x_train_data, y_train_data)\n",
    "            training_time = (utils.time_stamp() - training_time).total_seconds()\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Evaluating model: {}\".format(model[\"model_name\"]))\n",
    "\n",
    "            # Evaluation\n",
    "            y_predict = model[\"model_object\"].predict(x_valid)\n",
    "            performance = classification_report(y_valid, y_predict, output_dict = True)\n",
    "\n",
    "            # Assign model's perfomance\n",
    "            model[\"performance\"] = performance\n",
    "            model[\"f1_score_avg\"] = performance[\"macro avg\"][\"f1-score\"]\n",
    "            \n",
    "            # Create UID\n",
    "            uid = hashlib.md5(str(training_time).encode()).hexdigest()\n",
    "\n",
    "            # Assign model's UID\n",
    "            model[\"model_uid\"] = uid\n",
    "\n",
    "            # Collect current trained model\n",
    "            trained_model.append(copy.deepcopy(model))\n",
    "\n",
    "            # Debug message\n",
    "            utils.print_debug(\"Model {} has been trained for configuration data {}.\".format(model[\"model_name\"], config_data))\n",
    "        \n",
    "        # Collect current trained list of model\n",
    "        list_of_trained_model.append(copy.deepcopy(trained_model))\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"All combination models and configuration data has been trained.\")\n",
    "    \n",
    "    # Return list trained model\n",
    "    return list_of_trained_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(list_of_model):\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Making training log containing model UID and f1 macro avg.\")\n",
    "\n",
    "    # Make empty training log\n",
    "    training_log = {\"model_uid\": [], \"f1_score_avg\": []}\n",
    "    for model in list_of_model:\n",
    "        training_log[\"model_uid\"].append(model[\"model_uid\"])\n",
    "        training_log[\"f1_score_avg\"].append(model[\"f1_score_avg\"])\n",
    "\n",
    "    # Convert dictionary to pandas for easy operation\n",
    "    training_log = pd.DataFrame(training_log)\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Sorting training log by f1 macro avg.\")\n",
    "\n",
    "    # Sort training log by f1 score macro avg and trining time\n",
    "    best_model_log = training_log.sort_values(\"f1_score_avg\", ascending = False).iloc[0]\n",
    "\n",
    "    # Debug message\n",
    "    utils.print_debug(\"Searching model data based on sorted training log.\")\n",
    "\n",
    "    # Make best model variable\n",
    "    best_model = None\n",
    "\n",
    "    # Get model object with greatest f1 score macro avg by using UID\n",
    "    for model in list_of_model:\n",
    "        if model[\"model_uid\"] == best_model_log[\"model_uid\"]:\n",
    "            best_model = model\n",
    "            break\n",
    "    \n",
    "    # In case UID not found\n",
    "    if best_model == None:\n",
    "        raise RuntimeError(\"The best model not found in your list of model.\")\n",
    "    \n",
    "    # Debug message\n",
    "    utils.print_debug(\"Model chosen.\")\n",
    "    utils.print_debug(\"Model name: {}\".format(best_model[\"model_name\"]))\n",
    "    \n",
    "    # Return current chosen production model, log of production models and current training log\n",
    "    return best_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-05 20:20:05.303169 Training model based on configuration data: undersampling\n",
      "2022-12-05 20:20:05.303169 Creating model objects.\n",
      "2022-12-05 20:20:05.303169 Model objects created.\n",
      "2022-12-05 20:20:05.304176 Training model: LogisticRegression\n",
      "2022-12-05 20:20:09.737159 Evaluating model: LogisticRegression\n",
      "2022-12-05 20:20:09.746159 Model LogisticRegression has been trained for configuration data undersampling.\n",
      "2022-12-05 20:20:09.746159 Training model: DecisionTreeClassifier\n",
      "2022-12-05 20:20:10.074167 Evaluating model: DecisionTreeClassifier\n",
      "2022-12-05 20:20:10.085168 Model DecisionTreeClassifier has been trained for configuration data undersampling.\n",
      "2022-12-05 20:20:10.086163 Training model: RandomForestClassifier\n",
      "2022-12-05 20:20:22.482263 Evaluating model: RandomForestClassifier\n",
      "2022-12-05 20:20:22.562342 Model RandomForestClassifier has been trained for configuration data undersampling.\n",
      "2022-12-05 20:20:22.566325 Training model: KNeighborsClassifier\n",
      "2022-12-05 20:20:24.677240 Evaluating model: KNeighborsClassifier\n",
      "2022-12-05 20:20:24.764265 Model KNeighborsClassifier has been trained for configuration data undersampling.\n",
      "2022-12-05 20:20:24.765264 Training model: XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-05 20:20:31.077233 Evaluating model: XGBClassifier\n",
      "2022-12-05 20:20:31.120235 Model XGBClassifier has been trained for configuration data undersampling.\n",
      "2022-12-05 20:20:32.294256 All combination models and configuration data has been trained.\n"
     ]
    }
   ],
   "source": [
    "trained_models = train_eval(x_train, y_train, x_valid, y_valid, \"undersampling\", hyperparams_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-05 20:21:06.045662 Making training log containing model UID and f1 macro avg.\n",
      "2022-12-05 20:21:06.046678 Sorting training log by f1 macro avg.\n",
      "2022-12-05 20:21:06.047680 Searching model data based on sorted training log.\n",
      "2022-12-05 20:21:06.048669 Model chosen.\n",
      "2022-12-05 20:21:06.048669 Model name: LogisticRegression\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.512048</td>\n",
       "      <td>0.664179</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>0.588114</td>\n",
       "      <td>0.600964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.622378</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>0.589877</td>\n",
       "      <td>0.595368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.533752</td>\n",
       "      <td>0.642599</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>0.588176</td>\n",
       "      <td>0.597370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>305.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>0.595368</td>\n",
       "      <td>734.000000</td>\n",
       "      <td>734.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1  accuracy   macro avg  weighted avg\n",
       "precision    0.512048    0.664179  0.595368    0.588114      0.600964\n",
       "recall       0.557377    0.622378  0.595368    0.589877      0.595368\n",
       "f1-score     0.533752    0.642599  0.595368    0.588176      0.597370\n",
       "support    305.000000  429.000000  0.595368  734.000000    734.000000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = get_best_model(trained_models)\n",
    "performance = pd.DataFrame(best_model[\"performance\"])\n",
    "performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'saga', 'penalty': 'l1', 'max_iter': 600, 'C': 0.2}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model[\"model_object\"].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after exhaustive fitting, we still cannot find one that performs significantly better than our baseline.\n",
    "\n",
    "Because of what our EDA suggests, we should try to find some other features that didn't included here but available in our source dataset. Hopefully we can make a better model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
